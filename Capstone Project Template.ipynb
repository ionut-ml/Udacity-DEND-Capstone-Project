{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "*--describe your project at a high level--*\n",
    "\n",
    "This project uses datasets on US Immigration, Demographics and Global Temperatures. The goal of the project is to combine the skills learned throught the Udacity Data Engineering Nano-degree and prepare an ETL pipeline and a Data Model to be used for analytics.\n",
    "We do this by gathering and exploring source data in order to determine a proper Data Model that suits our purposes. Then we build a data pipeline that extracts the source information, transforms it by various clean-up steps, the loads it into the final tables of our defined data model.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import isnan, when, count, avg, col, udf, dayofmonth, dayofweek, month, year, weekofyear, monotonically_increasing_id, desc\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('credentials.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "*Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>*\n",
    "\n",
    "In this project we plan to use the datasets on US Immigration, Demographics and Global Temperatures as source files, develop a data pipeline in order to prepare them for loading into a data lake stored as parquet files on AWS S3 using a Data Model that we will define along the way.\n",
    "\n",
    "With this scope in hand, the main phases of the development will be the following:\n",
    "1. **Load** the datasets and do some **Exploratory Data Analysis**.\n",
    "    \n",
    "    This will do two things: first, allows us to understand the data, which will build our intuition for what the final Data Model will be, and secondly, it will start revealing the necessary steps in our Data Pipeline (e.g.: how to load each dataset, what clean-up steps are needed, what flaws we find, what data quality checks might be needed, etc.)\n",
    "\n",
    "\n",
    "2. Perform **clean-up steps** on each of the datasets.\n",
    "    \n",
    "    With the knowledge gathered in the previous phase, we start actually implementing cleaning steps and check further each datasets for potential quality issues. This will be the basis for some functions that will be used in our Data Pipeline.\n",
    "    \n",
    "\n",
    "3. **Define our Data Model**.\n",
    "    \n",
    "    In this phase, we will think more closely about how the final Data Model will look like, what will be the goal for it, what will be our fact and dimension tables, what schema will we use.\n",
    "    As we will see below, it will be shown that a slightly modified variant of the classic star schema will best suit our needs of providing a simple, easy-to-understand model that also allows for efficient queries for analytical purposes.\n",
    "    \n",
    "\n",
    "4. Build the **Data Pipeline** and perform **Data Quality Checks**.\n",
    "    \n",
    "    In this phase, we use the knowledge and steps identified earlier in order to build functions that will extract data from the source files, transform it by various clean-up steps, then load it into our fact and dimension tables previously defined.\n",
    "    Finally, we perform some simple data quality checks in order to ensure that our data pipeline ran as expected.\n",
    "\n",
    "To this end the technologies that we used are: python, Spark, AWS, Jupyter Notebooks.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "*Describe the data sets you're using. Where did it come from? What type of information is included?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1. I94 Immigration Data\n",
    "\n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data Dictionary:\n",
    "\n",
    "* **cicid**: Unique record ID\n",
    "* **i94yr**: 4 digit year\n",
    "* **i94mon**: Numeric month\n",
    "* **i94cit**: 3 digit code for immigrant country of birth\n",
    "* **i94res**: 3 digit code for immigrant country of residence\n",
    "* **i94port**: Port of admission\n",
    "* **arrdate**: Arrival Date in the USA\n",
    "* **i94mode**: Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)\n",
    "* **i94addr**: USA State of arrival\n",
    "* **depdate**: Departure Date from the USA\n",
    "* **i94bir**: Age of Respondent in Years\n",
    "* **i94visa**: Visa codes collapsed into three categories\n",
    "* **count**: Field used for summary statistics\n",
    "* **dtadfile**: Character Date Field - Date added to I-94 Files\n",
    "* **visapost**: Department of State where where Visa was issued\n",
    "* **occup**: Occupation that will be performed in U.S\n",
    "* **entdepa**: Arrival Flag - admitted or paroled into the U.S.\n",
    "* **entdepd**: Departure Flag - Departed, lost I-94 or is deceased\n",
    "* **entdepu**: Update Flag - Either apprehended, overstayed, adjusted to perm residence\n",
    "* **matflag**: Match flag - Match of arrival and departure records\n",
    "* **biryear**: 4 digit year of birth\n",
    "* **dtaddto**: Character Date Field - Date to which admitted to U.S. (allowed to stay until)\n",
    "* **gender**: Non-immigrant sex\n",
    "* **insnum**: INS number\n",
    "* **airline**: Airline used to arrive in U.S.\n",
    "* **admnum**: Admission Number\n",
    "* **fltno**: Flight number of Airline used to arrive in U.S.\n",
    "* **visatype**: lass of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "i94_data = spark.read.load('./sas_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>QF</td>\n",
       "      <td>9.495387e+10</td>\n",
       "      <td>00011</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>VA</td>\n",
       "      <td>9.495562e+10</td>\n",
       "      <td>00007</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495641e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495645e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495639e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0  5748517.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "1  5748518.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "2  5748519.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "3  5748520.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "4  5748521.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "\n",
       "  i94addr  depdate   ...     entdepu  matflag  biryear   dtaddto gender  \\\n",
       "0      CA  20582.0   ...        None        M   1976.0  10292016      F   \n",
       "1      NV  20591.0   ...        None        M   1984.0  10292016      F   \n",
       "2      WA  20582.0   ...        None        M   1987.0  10292016      M   \n",
       "3      WA  20588.0   ...        None        M   1987.0  10292016      F   \n",
       "4      WA  20588.0   ...        None        M   1988.0  10292016      M   \n",
       "\n",
       "  insnum airline        admnum  fltno visatype  \n",
       "0   None      QF  9.495387e+10  00011       B1  \n",
       "1   None      VA  9.495562e+10  00007       B1  \n",
       "2   None      DL  9.495641e+10  00040       B1  \n",
       "3   None      DL  9.495645e+10  00040       B1  \n",
       "4   None      DL  9.495639e+10  00040       B1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i94_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.\\\n",
    "# config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "# config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "# enableHiveSupport().getOrCreate()\n",
    "\n",
    "# df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# #write to parquet\n",
    "# df_spark.write.parquet(\"sas_data\")\n",
    "# df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2. Temperature Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This dataset came from Kaggle. From their dataset description:\n",
    "*We have repackaged the data from a newer compilation put together by the [Berkeley Earth](http://berkeleyearth.org/about/), which is affiliated with Lawrence Berkeley National Laboratory. The Berkeley Earth Surface Temperature Study combines 1.6 billion temperature reports from 16 pre-existing archives. It is nicely packaged and allows for slicing into interesting subsets (for example by country). They publish the source data and the code for the transformations they applied. They also use methods that allow weather observations from shorter time series to be included, meaning fewer observations need to be thrown away.*\n",
    "\n",
    "The raw data comes from the [Berkeley Earth data page](http://berkeleyearth.org/data/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data Dictionary:\n",
    "\n",
    "* **dt**: Date\n",
    "* **AverageTemperature**: Global average land temperature in celsius\n",
    "* **AverageTemperatureUncertainty**: 95% confidence interval around the average\n",
    "* **City**: Name of City\n",
    "* **Country**: Name of Country\n",
    "* **Latitude**: City Latitude\n",
    "* **Longitude**: City Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temp_df = spark.read.csv(fname, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0 1743-11-01               6.068                          1.737  Århus   \n",
       "1 1743-12-01                 NaN                            NaN  Århus   \n",
       "2 1744-01-01                 NaN                            NaN  Århus   \n",
       "3 1744-02-01                 NaN                            NaN  Århus   \n",
       "4 1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 3. U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This data comes from OpenSoft.\n",
    "\n",
    "This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. \n",
    "\n",
    "This data comes from the US Census Bureau's 2015 American Community Survey.\n",
    "\n",
    "Reference: https://www.census.gov/data/developers/about/terms-of-service.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data Dictionary:\n",
    "\n",
    "* **City**: City Name\n",
    "* **State**: US State where city is located\n",
    "* **Median Age**: Median age of the population\n",
    "* **Male Population**: Count of male population\n",
    "* **Female Population**: Count of female population\n",
    "* **Total Population**: Count of total population\n",
    "* **Number of Veterans**: Count of total Veterans\n",
    "* **Foreign born**: Count of residents of the city that were not born in the city\n",
    "* **Average Household Size**: Average city household size\n",
    "* **State Code**: Code of the US state\n",
    "* **Race**: Respondent race\n",
    "* **Count**: Count of city's individual per race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = 'us-cities-demographics.csv'\n",
    "demog_df = spark.read.csv(fname, header=True, inferSchema=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demog_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8            40601   \n",
       "1            Quincy  Massachusetts        41.0            44129   \n",
       "2            Hoover        Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga     California        34.5            88127   \n",
       "4            Newark     New Jersey        34.6           138040   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0              41862             82463                1562         30908   \n",
       "1              49500             93629                4147         32935   \n",
       "2              46799             84839                4819          8229   \n",
       "3              87105            175232                5821         33878   \n",
       "4             143873            281913                5829         86253   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demog_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "*Identify data quality issues, like missing values, duplicate data, etc.*\n",
    "\n",
    "#### Cleaning Steps\n",
    "*Document steps necessary to clean the data:*\n",
    "* drop columns with over 70% missing values;\n",
    "* drop rows with missing data, taking into account unique identifier for row per dataset;\n",
    "* drop duplicates;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 1. I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "total_row_count = i94_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "col_nulls_df = i94_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in i94_data.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "col_nulls_df = pd.melt(col_nulls_df, var_name='Column Name', value_name='Null Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "col_nulls_df['Ratio to total'] = (col_nulls_df['Null Count']/total_row_count).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Ratio to total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>occup</td>\n",
       "      <td>3088187</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>entdepu</td>\n",
       "      <td>3095921</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>insnum</td>\n",
       "      <td>2982605</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column Name  Null Count  Ratio to total\n",
       "15       occup     3088187           0.997\n",
       "18     entdepu     3095921           1.000\n",
       "23      insnum     2982605           0.963"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_nulls_df.loc[col_nulls_df['Ratio to total'] > 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols = ['occup', 'entdepu', 'insnum']\n",
    "\n",
    "i94_data = i94_data.drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_data = i94_data.dropDuplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_data = i94_data.dropna(how='all', subset=['cicid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2. Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df.dropna(how='all', subset=['AverageTemperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df.dropDuplicates(['dt', 'City', 'Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 3. U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demog_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df = demog_df.dropna(how='all', subset=['City', 'Male Population', 'Female Population', 'Foreign-born', 'Average Household Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df = demog_df.dropDuplicates(['City', 'Male Population', 'Female Population', 'Foreign-born', 'Average Household Size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "*Map out the conceptual data model and explain why you chose that model*\n",
    "\n",
    "In order to optimize for simple, easy-to-understand, efficient, queries for analytical use of the Database, the star schema is the best solution for this scope. We also applied a small change to the classic star schema to include the Temperatures data with a relationship to the US Cities.\n",
    "\n",
    "![Alt text](./Udacity-DEND-Capstone-ERD.svg)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "*List the steps necessary to pipeline the data into the chosen data model*\n",
    "\n",
    "The pipeline has the following steps:\n",
    "1. Extract the data from the .CSV or parquet source files.\n",
    "2. Transform, or clean the data by dropping duplicates, nulls and, optionally, columns with mostly or only nulls.\n",
    "3. Create the 4 dimension tables from the I94 Immigration, USA Demographics & Temparatures datasets by selecting required column, adjusting names and data types, and extracting other information (e.g.: data/time formats).\n",
    "4. Create the i94 immigration fact table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create functions needed in processing the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Creates Spark Session to be used for data processing.\n",
    "    \"\"\"\n",
    "    \n",
    "#     spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "    os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "    os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_dataset(df, subset=None, drop_cols=False, thresh=0.7):\n",
    "    \"\"\"\n",
    "    Cleans-up dataset by dropping duplicates and NaN's/Nulls.\n",
    "    Optionally, it drops columns with high % of nulls.\n",
    "    Input:\n",
    "        df: Spark DataFrame\n",
    "        subset: default None. List of column names to take into account.\n",
    "        drop_cols: default False. Whether to drop columns with high % of Nulls.\n",
    "        thresh: default 0.7. Threshold for deciding % of nulls of column to drop.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.dropna(how='all', subset=subset)\n",
    "    df = df.dropDuplicates(subset)\n",
    "    \n",
    "    if drop_cols:\n",
    "        total_row_count = df.count()\n",
    "        col_nulls_df = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas()\n",
    "        col_nulls_df = pd.melt(col_nulls_df, var_name='Column Name', value_name='Null Count')\n",
    "        col_nulls_df['Ratio to total'] = (col_nulls_df['Null Count']/total_row_count).round(3)\n",
    "        cols = col_nulls_df.loc[col_nulls_df['Ratio to total'] > 0.7].columns.tolist()\n",
    "        df = df.drop(*cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dim_calendar_table(df, output_location):\n",
    "    \"\"\"\n",
    "    Creates the dim_calendar table for the data model.\n",
    "    Input:\n",
    "        df: Spark DataFrame.\n",
    "        output_location: String. Directory or S3 location to write the parquet file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # udf to convert dt into a datetime data type.\n",
    "    get_dt = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    # create initial dataFrame, extract required columns, date & time formats.\n",
    "    calendar = df.select(['arrdate']).withColumn(\"arrdate\", get_dt(df.arrdate)).distinct()\n",
    "    calendar = calendar.withColumn(\"id\", monotonically_increasing_id())\n",
    "    calendar = calendar.withColumn(\"year\", year(\"arrdate\"))\n",
    "    calendar = calendar.withColumn(\"month\", month(\"arrdate\"))\n",
    "    calendar = calendar.withColumn(\"day\", dayofmonth(\"arrdate\"))\n",
    "    calendar = calendar.withColumn(\"week\", weekofyear(\"arrdate\"))\n",
    "    calendar = calendar.withColumn(\"weekday\", dayofweek(\"arrdate\"))\n",
    "    \n",
    "    # write to parquet file\n",
    "    partition_cols = [\"year\", \"month\"]\n",
    "    calendar.write.parquet(output_location + \"dim_calendar\", partitionBy=partition_cols, mode=\"overwrite\")\n",
    "    \n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_location = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calendar = create_dim_calendar_table(i94_data, output_location=output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----+-----+---+----+-------+\n",
      "|   arrdate|         id|year|month|day|week|weekday|\n",
      "+----------+-----------+----+-----+---+----+-------+\n",
      "|2016-04-22| 8589934592|2016|    4| 22|  16|      6|\n",
      "|2016-04-15|25769803776|2016|    4| 15|  15|      6|\n",
      "|2016-04-18|42949672960|2016|    4| 18|  16|      2|\n",
      "|2016-04-09|68719476736|2016|    4|  9|  14|      7|\n",
      "|2016-04-11|85899345920|2016|    4| 11|  15|      2|\n",
      "+----------+-----------+----+-----+---+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calendar.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dim_usa_demographics_table(df, output_location):\n",
    "    \"\"\"\n",
    "    Creates the dim_usa_demographics table for the data model.\n",
    "    Input:\n",
    "        df: Spark DataFrame.\n",
    "        output_location: String. Directory or S3 location to write the parquet file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create DataFrame with required column names\n",
    "    usdemog_df = df.withColumnRenamed(\"City\", \"city\")\\\n",
    "                .withColumnRenamed(\"State\", \"state\") \\\n",
    "                .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "                .withColumnRenamed(\"Male Population\", \"male_pop\") \\\n",
    "                .withColumnRenamed(\"Female Population\", \"female_pop\") \\\n",
    "                .withColumnRenamed(\"Total Population\", \"total_pop\") \\\n",
    "                .withColumnRenamed(\"Number of Veterans\", \"veteran_number\") \\\n",
    "                .withColumnRenamed(\"Foreign-born\", \"foreign_born\") \\\n",
    "                .withColumnRenamed(\"Average Household Size\", \"avg_household_size\") \\\n",
    "                .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "                .withColumnRenamed(\"Race\", \"race\") \\\n",
    "                .withColumnRenamed(\"Count\", \"count\")\n",
    "    \n",
    "    # add id column\n",
    "    usdemog_df = usdemog_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "    \n",
    "    # write parquet file\n",
    "    usdemog_df.write.parquet(output_location + \"dim_usa_demographics\", mode=\"overwrite\")\n",
    "    \n",
    "    return usdemog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_demographics = create_dim_usa_demographics_table(demog_df, output_location=output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+--------+----------+---------+--------------+------------+------------------+----------+--------------------+-----+---+\n",
      "|            city|        state|median_age|male_pop|female_pop|total_pop|veteran_number|foreign_born|avg_household_size|state_code|                race|count| id|\n",
      "+----------------+-------------+----------+--------+----------+---------+--------------+------------+------------------+----------+--------------------+-----+---+\n",
      "|   Silver Spring|     Maryland|      33.8|   40601|     41862|    82463|          1562|       30908|               2.6|        MD|  Hispanic or Latino|25924|  0|\n",
      "|          Quincy|Massachusetts|      41.0|   44129|     49500|    93629|          4147|       32935|              2.39|        MA|               White|58723|  1|\n",
      "|          Hoover|      Alabama|      38.5|   38040|     46799|    84839|          4819|        8229|              2.58|        AL|               Asian| 4759|  2|\n",
      "|Rancho Cucamonga|   California|      34.5|   88127|     87105|   175232|          5821|       33878|              3.18|        CA|Black or African-...|24437|  3|\n",
      "|          Newark|   New Jersey|      34.6|  138040|    143873|   281913|          5829|       86253|              2.73|        NJ|               White|76402|  4|\n",
      "+----------------+-------------+----------+--------+----------+---------+--------------+------------+------------------+----------+--------------------+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_demographics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dim_immigrant_table(df, output_location):\n",
    "    \"\"\"\n",
    "    Creates the dim_immigrant table for the data model.\n",
    "    Input:\n",
    "        df: Spark DataFrame.\n",
    "        output_location: String. Directory or S3 location to write the parquet file. \n",
    "    \"\"\"\n",
    "    \n",
    "    # create a DataFrame with required columns.\n",
    "    cols = [\n",
    "        \"cicid\"\n",
    "        , \"i94cit\"\n",
    "        , \"i94res\"\n",
    "        , \"i94mode\"\n",
    "        , \"depdate\"\n",
    "        , \"i94bir\"\n",
    "        , \"gender\"\n",
    "    ]\n",
    "    im_df = df.select(cols)\n",
    "    \n",
    "    # write parquet file\n",
    "    im_df.write.parquet(output_location + \"dim_immigrant\", mode=\"overwrite\")\n",
    "    \n",
    "    return im_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immig_df = create_dim_immigrant_table(i94_data, output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-------+-------+------+------+\n",
      "|    cicid|i94cit|i94res|i94mode|depdate|i94bir|gender|\n",
      "+---------+------+------+-------+-------+------+------+\n",
      "|5748517.0| 245.0| 438.0|    1.0|20582.0|  40.0|     F|\n",
      "|5748518.0| 245.0| 438.0|    1.0|20591.0|  32.0|     F|\n",
      "|5748519.0| 245.0| 438.0|    1.0|20582.0|  29.0|     M|\n",
      "|5748520.0| 245.0| 438.0|    1.0|20588.0|  29.0|     F|\n",
      "|5748521.0| 245.0| 438.0|    1.0|20588.0|  28.0|     M|\n",
      "+---------+------+------+-------+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immig_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dim_temperatures_table(df, output_location):\n",
    "    \"\"\"\n",
    "    Creates the dim_temperatures table for the data model.\n",
    "    Input:\n",
    "        df: Spark DataFrame.\n",
    "        output_location: String. Directory or S3 location to write the parquet file. \n",
    "    \"\"\"\n",
    "    \n",
    "    # create a DataFrame with required columns.\n",
    "    temp_df = df.withColumnRenamed(\"dt\", \"dt\") \\\n",
    "                .withColumnRenamed(\"AverageTemperature\", \"avg_temp\") \\\n",
    "                .withColumnRenamed(\"AverageTemperatureUncertainty\", \"avg_temp_uncertainty\") \\\n",
    "                .withColumnRenamed(\"City\", \"city\") \\\n",
    "                .withColumnRenamed(\"Country\", \"country\") \\\n",
    "                .withColumnRenamed(\"Latitude\", \"latitude\") \\\n",
    "                .withColumnRenamed(\"Longitude\", \"longitude\")\n",
    "    \n",
    "    temp_df.write.parquet(output_location + \"dim_temperatures\", mode=\"overwrite\")\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dimTemp_df = create_dim_temperatures_table(temp_df, output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------+-----+-------+--------+---------+\n",
      "|                 dt|avg_temp|avg_temp_uncertainty| city|country|latitude|longitude|\n",
      "+-------------------+--------+--------------------+-----+-------+--------+---------+\n",
      "|1743-11-01 00:00:00|   6.068|  1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01 00:00:00|    null|                null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01 00:00:00|    null|                null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01 00:00:00|    null|                null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01 00:00:00|    null|                null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+-------------------+--------+--------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dimTemp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create I94 Immigration Fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_fact_i94_immigration_table(df, output_location):\n",
    "    \"\"\"\n",
    "    Creates the fact_i94_immigration table for the data model.\n",
    "    Input:\n",
    "        df: Spark DataFrame.\n",
    "        output_location: String. Directory or S3 location to write the parquet file. \n",
    "    \"\"\"\n",
    "    \n",
    "    # udf to convert dt into a datetime data type.\n",
    "    get_dt = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "        \n",
    "    # create a DataFrame with required columns.\n",
    "    cols = [\n",
    "        \"cicid\"\n",
    "        , \"arrdate\"\n",
    "        , \"count\"\n",
    "        , \"visapost\"\n",
    "        , \"entdepa\"\n",
    "        , \"entdepd\"\n",
    "        , \"biryear\"\n",
    "        , \"dtaddto\"\n",
    "        , \"airline\"\n",
    "        , \"fltno\"\n",
    "        , \"visatype\"\n",
    "        , \"i94addr\"\n",
    "        , \"i94visa\"\n",
    "    ]\n",
    "    i94_df = df.select(cols)\n",
    "    \n",
    "    # convert arrdate column into Datatime data type\n",
    "    i94_df = i94_df.withColumn(\"arrdate\", get_dt(df.arrdate))\n",
    "    \n",
    "    # write parquet file\n",
    "    i94_df.write.parquet(output_location + \"fact_i94_immigration\", mode=\"overwrite\")\n",
    "    \n",
    "    return i94_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "factI94_df = create_fact_i94_immigration_table(i94_data, output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+--------+-------+-------+-------+--------+-------+-----+--------+-------+-------+\n",
      "|    cicid|   arrdate|count|visapost|entdepa|entdepd|biryear| dtaddto|airline|fltno|visatype|i94addr|i94visa|\n",
      "+---------+----------+-----+--------+-------+-------+-------+--------+-------+-----+--------+-------+-------+\n",
      "|5748517.0|2016-04-30|  1.0|     SYD|      G|      O| 1976.0|10292016|     QF|00011|      B1|     CA|    1.0|\n",
      "|5748518.0|2016-04-30|  1.0|     SYD|      G|      O| 1984.0|10292016|     VA|00007|      B1|     NV|    1.0|\n",
      "|5748519.0|2016-04-30|  1.0|     SYD|      G|      O| 1987.0|10292016|     DL|00040|      B1|     WA|    1.0|\n",
      "|5748520.0|2016-04-30|  1.0|     SYD|      G|      O| 1987.0|10292016|     DL|00040|      B1|     WA|    1.0|\n",
      "|5748521.0|2016-04-30|  1.0|     SYD|      G|      O| 1988.0|10292016|     DL|00040|      B1|     WA|    1.0|\n",
      "+---------+----------+-----+--------+-------+-------+-------+--------+-------+-----+--------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "factI94_dfI94_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_i94_data(spark, input_location, output_location, datadir):\n",
    "    \"\"\"\n",
    "    ETL for the i94 immigration data. \n",
    "    Creates the dim_calendar, dim_immigrant and fact_i94_immigration tables.\n",
    "    Input:\n",
    "        spark: Spark Session.\n",
    "        input_location: String. Directory or S3 location to read file.\n",
    "        output_location: String. Directory or S3 location to write the parquet file. \n",
    "        datadir: String. Directory with parquet files with source data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read files into Spark DataFrame\n",
    "    i94_data = spark.read.load(datadir)\n",
    "    \n",
    "    # clean-up i94 data\n",
    "    i94_df = clean_dataset(i94_data, subset=None, drop_cols=False, thresh=0.7)\n",
    "    \n",
    "    # create dim_calendar table and save to parquet file in output location\n",
    "    calendar_table = create_dim_calendar_table(i94_df, output_location)\n",
    "    \n",
    "    # create dim_immigrant table and save to parquet file in output location\n",
    "    im_table = create_dim_immigrant_table(i94_df, output_location)\n",
    "    \n",
    "    # create fact_i94_immigration table and save to parquet file in output location\n",
    "    i94_table = create_fact_i94_immigration_table(i94_df, output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_usa_demog_data(spark, input_location, output_location, fname):\n",
    "    \"\"\"\n",
    "    ETL for the USA Demographics data. \n",
    "    Creates the dim_calendar, dim_immigrant and fact_i94_immigration tables.\n",
    "    Input:\n",
    "        spark: Spark Session.\n",
    "        input_location: String. Directory or S3 location to read file.\n",
    "        output_location: String. Directory or S3 location to write the parquet file. \n",
    "        fname: String. Name of files with source data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read file into Spark DataFrame\n",
    "    demog_data = spark.read.csv(fname, header=True, inferSchema=True, sep=';')\n",
    "    \n",
    "    # clean-up demographics data\n",
    "    demog_df = clean_dataset(demog_data, subset=None, drop_cols=False, thresh=0.7)\n",
    "    \n",
    "    # create dim_usa_demographics table and save to parquet file in output location\n",
    "    demog_table = create_dim_usa_demographics_table(demog_df, output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_temperatures_data(spark, input_location, output_location, fname):\n",
    "    \"\"\"\n",
    "    ETL for the Tables data. \n",
    "    Creates the dim_calendar, dim_immigrant and fact_i94_immigration tables.\n",
    "    Input:\n",
    "        spark: Spark Session.\n",
    "        input_location: String. Directory or S3 location to read file.\n",
    "        output_location: String. Directory or S3 location to write the parquet file. \n",
    "        fname: String. Name of files with source data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read file into Spark DataFrame\n",
    "    temp_data = spark.read.csv(fname, header=True, inferSchema=True, sep=';')\n",
    "    \n",
    "    # clean-up demographics data\n",
    "    temp_df = clean_dataset(temp_data, subset=None, drop_cols=False, thresh=0.7)\n",
    "    \n",
    "    # create dim_usa_demographics table and save to parquet file in output location\n",
    "    temp_table = create_dim_temperatures_table(temp_df, output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "*Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:*\n",
    " * *Integrity constraints on the relational database (e.g., unique key, data type, etc.)*\n",
    " * *Unit tests for the scripts to ensure they are doing the right thing*\n",
    " * *Source/Count checks to ensure completeness*\n",
    "\n",
    "The data quality checks performed on the final datasets in order to ensure that the pipeline ran correctly and as expected are the following:\n",
    "* Check total number of rows in dataset. If no rows are loaded, it means that the pipeline failed.\n",
    "* Check number of duplicates. If it is greater than 0, it means that the data clean-up did not work correctly and we have duplicate data in our tables.\n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_quality_checks(df):\n",
    "    \"\"\"\n",
    "    Runs data quality checks against newly created table in the data model.\n",
    "    Input:\n",
    "        df: Spark DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_rows = df.count()\n",
    "    no_duplicate_rows = df.dropDuplicates(how='all').count()\n",
    "    total_duplicates = total_rows - no_duplicate_rows\n",
    "    \n",
    "    if total_rows == 0:\n",
    "        return \"Data quality check failed. Table has 0 records.\"\n",
    "    else:\n",
    "        if total_duplicates == 0:\n",
    "            return f\"Data quality check failed. Table has {total_duplicates} duplicates.\"\n",
    "        else:\n",
    "            return f\"Data quality check passed. Table has {total_rows} and no duplicates.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "*Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.*\n",
    "\n",
    "**Dimension Tables**:\n",
    "\n",
    "1. **dim_immigrant**:\n",
    "    * ccid: Unique record ID\n",
    "    * i94cit: 3 digit code for immigrant country of birth\n",
    "    * i94res: 3 digit code for immigrant country of residence\n",
    "    * i94mode: Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)\n",
    "    * depdate: Departure Date from the USA\n",
    "    * i94bir: Age of Respondent in Years\n",
    "    * gender: Non-immigrant sex\n",
    "2. **dim_usa_demographics**:\n",
    "    * id: Unique record ID\n",
    "    * city: City Name\n",
    "    * state: US State where city is located\n",
    "    * median_age: Median age of the population\n",
    "    * male_pop: Count of male population\n",
    "    * female_pop: Count of female population\n",
    "    * total_pop: Count of total population\n",
    "    * veteran_number: Count of total Veterans\n",
    "    * foreign_born: Count of residents of the city that were not born in the city\n",
    "    * avg_household_size: Average city household size\n",
    "    * race: Respondent race\n",
    "    * count: Count of city's individual per race\n",
    "3. **dim_temperatures**:\n",
    "    * dt: Date\n",
    "    * avg_temp: Global average land temperature in celsius\n",
    "    * avg_temp_uncertainty: 95% confidence interval around the average\n",
    "    * city: Name of City\n",
    "    * country: Name of Country\n",
    "    * latitude: City Latitude\n",
    "    * longitude: City Longitude\n",
    "4. **dim_calendar**:\n",
    "    * arrdate: Arrival Date in the USA\n",
    "    * year: 4 digit year\n",
    "    * month: Numeric month of year\n",
    "    * day: Numeric day of month\n",
    "    * week: Numeric week of year\n",
    "    * weekday: Day of week.\n",
    "\n",
    "**Fact Table**:\n",
    "1. **fact_i94_immigration**:\n",
    "    * ccid: Unique record ID\n",
    "    * arrdate: Arrival Date in the USA\n",
    "    * count: Field used for summary statistics\n",
    "    * visapost: Department of State where where Visa was issued\n",
    "    * entdepa: Arrival Flag - admitted or paroled into the U.S.\n",
    "    * entdepd: Departure Flag - Departed, lost I-94 or is deceased\n",
    "    * biryear: 4 digit year of birth\n",
    "    * dtaddto: \tCharacter Date Field - Date to which admitted to U.S. (allowed to stay until)\n",
    "    * airline: \tAirline used to arrive in U.S.\n",
    "    * fltno: Flight number of Airline used to arrive in U.S.\n",
    "    * visatype: Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "    * i94addr: USA State of arrival\n",
    "    * i94visa: Visa codes collapsed into three categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* *What is target audience, who is going to utilize the final data model?*\n",
    "    \n",
    "    The final data lake will be best suited for an analytics team with familiarity of the technologies used and the skills to connect, analyze and even visualize the resulting insights. \n",
    "    \n",
    "    \n",
    "* *What are some of the types of questions we can handle using the proposed data model?, Please be specific as we also need to show the output in the notebook at the end to validate if the data model is able to answer the questions*\n",
    "    \n",
    "    This Data Model will allow us to answer questions such as: \n",
    "        - What are the 10 US states with the most immigrants arriving?\n",
    "        - What are the 10 countries of birth with the most immigrants to the US?\n",
    "        - What is the gender distribution? \n",
    "        - What is the average age?\n",
    "\n",
    "\n",
    "* *Clearly state the rationale for the choice of tools and technologies for the project.*\n",
    "    \n",
    "    The reason for using Apache Spark is its scalability, and with its pyspark implementation the ease-of-use and easy data manipulation capabilities. It allows us to build a pipeline using initial smaller datasets, and it will be perfectly capable to scale and perform efficiently when the data increases.\n",
    "    Jupyter Notebooks are used for interactivity and fast data exploration and in order to build the initial version of the pipeline.\n",
    "    AWS, specifically S3, is one of the best solution on the market for storage, and as with Spark, it's easily scalable. We will use S3 to store our final data lake, in parquet files.\n",
    "    \n",
    "    \n",
    "* *Propose how often the data should be updated and why.*\n",
    "    \n",
    "    The main factor that determines the data refresh rate is how often do the source datasets are updated. Since the Temperatures and I94 Immigration datasets are updated monthly, this should be what we must aim for.\n",
    "    \n",
    "    \n",
    "* *Write a description of how you would approach the problem differently under the following scenarios:*\n",
    " * *The data was increased by 100x.* \n",
    "     We could use a cluster manager like Yarn or spin-up an EMR cluster that can scale our compute resourses. Spark can then handle technology-wise the scale of the data. Also, on the storage front, S3 can scale up or down according to the data lake size.\n",
    "     \n",
    " * *The data populates a dashboard that must be updated on a daily basis by 7am every day.*\n",
    "     The ETL process would be handled and scheduled by a tool like Apache Airflow. This will allows us to set-up the refresh times on a daily basis.  \n",
    " * *The database needed to be accessed by 100+ people.*\n",
    "     We can increase our nodes using Yarn or EMR, or, alternatively, move our data lake to a cloud, managed Database like Redshfit that is designed for analytical purposes, has very fast read times, and can handle high workloads (users performing queries on the DB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### What are the 10 US states with the most immigrants arriving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|i94addr|sum(count)|\n",
      "+-------+----------+\n",
      "|     FL|  621701.0|\n",
      "|     NY|  553677.0|\n",
      "|     CA|  470386.0|\n",
      "|     HI|  168764.0|\n",
      "|   null|  152592.0|\n",
      "|     TX|  134321.0|\n",
      "|     NV|  114609.0|\n",
      "|     GU|   94107.0|\n",
      "|     IL|   82126.0|\n",
      "|     NJ|   76531.0|\n",
      "+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "factI94_df.groupBy(\"i94addr\").sum(\"count\").sort(desc(\"sum(count)\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|i94addr| count|\n",
      "+-------+------+\n",
      "|     FL|621701|\n",
      "|     NY|553677|\n",
      "|     CA|470386|\n",
      "|     HI|168764|\n",
      "|   null|152592|\n",
      "|     TX|134321|\n",
      "|     NV|114609|\n",
      "|     GU| 94107|\n",
      "|     IL| 82126|\n",
      "|     NJ| 76531|\n",
      "+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "factI94_df.groupBy(\"i94addr\").count().sort(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### What are the 10 countries of birth with the most immigrants to the US?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|i94cit| count|\n",
      "+------+------+\n",
      "| 135.0|360157|\n",
      "| 209.0|206873|\n",
      "| 245.0|191425|\n",
      "| 111.0|188766|\n",
      "| 582.0|175781|\n",
      "| 148.0|157806|\n",
      "| 254.0|137735|\n",
      "| 689.0|129833|\n",
      "| 213.0|110691|\n",
      "| 438.0|109884|\n",
      "+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immig_df.groupBy(\"i94cit\").count().sort(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### What is the gender distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     M|1377224|\n",
      "|     F|1302743|\n",
      "|  null| 414269|\n",
      "|     X|   1610|\n",
      "|     U|    467|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immig_df.groupBy(\"gender\").count().sort(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### What is the average age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       avg(i94bir)|\n",
      "+------------------+\n",
      "|41.767614458485205|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immig_df.groupBy().avg().select(col(\"avg(i94bir)\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
